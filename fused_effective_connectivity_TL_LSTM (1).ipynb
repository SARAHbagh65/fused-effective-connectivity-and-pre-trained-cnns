{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq6mjSxueTtc"
      },
      "source": [
        "# **Initializers**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "n9a4bUVTfMhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_rCXefUExyn"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/My Drive/08_emotion/\n",
        "base_dir='/content/gdrive/My Drive/08_emotion/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFhFEoY7e2rM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "'''from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3'''\n",
        "from keras.applications import vgg16\n",
        "from keras.applications import xception\n",
        "from keras.applications import densenet\n",
        "from keras.applications import vgg19\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Layer,Dense, Dropout, Input, Activation, TimeDistributed, Reshape\n",
        "from tensorflow.keras.layers import  GRU, Bidirectional\n",
        "from tensorflow.keras.layers import  Conv1D, Conv2D, MaxPooling2D, Flatten, BatchNormalization, LSTM, ZeroPadding2D, GlobalAveragePooling2D, SpatialDropout2D, GlobalMaxPool1D,Convolution1D\n",
        "from tensorflow.keras.callbacks import History \n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def smooth_curve(points, factor=0.0):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "def plot_results(history):\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  %matplotlib inline\n",
        "  plt.plot(epochs, smooth_curve(acc), 'bo', label='Training acc')\n",
        "  plt.plot(epochs, smooth_curve(val_acc), 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, smooth_curve(loss), 'bo', label='Training loss')\n",
        "  plt.plot(epochs, smooth_curve(val_loss), 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def calculate_test_accuracy():\n",
        "  fold_results = {}\n",
        "  Y_test = np_utils.to_categorical(Y_test)\n",
        "  predicted_probas = model.predict(np.array(X_test),steps = len(X_test))\n",
        "  fold_results['predicted_probas'] = predicted_probas\n",
        "  binary_prediction=[]\n",
        "  binary_prediction0=[]\n",
        "  for i in range(len(predicted_probas)):\n",
        "     binary_prediction0 = 1 if predicted_probas[i]>=0.5 else 0\n",
        "     # binary_prediction0 = np.round(predicted_probas)\n",
        "     binary_prediction.append(binary_prediction0)\n",
        "  fold_results['binary_prediction'] = binary_prediction\n",
        "  conf_mat = confusion_matrix(binary_prediction, Y_test)\n",
        "  fold_results['confusion_matrix'] = conf_mat\n",
        "  acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
        "  fold_results['test_accuracy'] = acc\n",
        "  print('Overall accuracy: {:.2f} %'.format(acc*100))\n",
        "  return fold_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwrH5aaNeSFg"
      },
      "source": [
        "# **Load Connectivity images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG5bTqhIDDHt"
      },
      "outputs": [],
      "source": [
        "from natsort import natsorted\n",
        "import numpy as np\n",
        "import os\n",
        "base_dir1 = '/content/gdrive/My Drive/08_emotion/data/emotion-fused images/q1/'\n",
        "q1_data = os.listdir(base_dir1)\n",
        "q1_data = natsorted(q1_data)\n",
        "\n",
        "base_dir2 = '/content/gdrive/My Drive/08_emotion/data/emotion-fused images/q2/'\n",
        "q2_data = os.listdir(base_dir2)\n",
        "q2_data = natsorted(q2_data)\n",
        "\n",
        "base_dir3 = '/content/gdrive/My Drive/08_emotion/data/emotion-fused images/q3/'\n",
        "q3_data = os.listdir(base_dir3)\n",
        "q3_data = natsorted(q3_data)\n",
        "\n",
        "base_dir4 = '/content/gdrive/My Drive/08_emotion/data/emotion-fused images/q4/'\n",
        "q4_data = os.listdir(base_dir4)\n",
        "q4_data = natsorted(q4_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cvQWlNFEP18"
      },
      "outputs": [],
      "source": [
        "unwanted_num = {'fi(4626)q2 (1).jpg', 'fi(4627)q2 (1).jpg', 'fi(4628)q2 (1).jpg', 'fi(4629)q2 (1).jpg', 'fi(4630)q2 (1).jpg', 'fi(4631)q2 (1).jpg', 'fi(4632)q2 (1).jpg'}\n",
        "list1 = [ele for ele in q2_data if ele not in unwanted_num]\n",
        "q2_data=list1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8LRqUyVnEZxW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pickle\n",
        "base_dir = '/content/gdrive/My Drive/08_emotion/'\n",
        "\n",
        "X_q1 = []\n",
        "for img in q1_data:\n",
        "    image = cv2.imread(base_dir1+img)\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    X_q1.append(image)\n",
        "with open(base_dir+'q1', 'wb') as file_pi1:\n",
        "    pickle.dump(X_q1, file_pi1)\n",
        "    \n",
        "X_q2 = []\n",
        "for img in q2_data:\n",
        "    image = cv2.imread(base_dir2+img)\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    X_q2.append(image)\n",
        "with open(base_dir+'q2', 'wb') as file_pi1:\n",
        "    pickle.dump(X_q2, file_pi1)\n",
        "\n",
        "X_q3 = []\n",
        "for img in q3_data:\n",
        "    image = cv2.imread(base_dir3+img)\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    X_q3.append(image)\n",
        "with open(base_dir+'q3', 'wb') as file_pi1:\n",
        "    pickle.dump(X_q3, file_pi1)\n",
        "\n",
        "X_q4 = []\n",
        "for img in q4_data:\n",
        "    image = cv2.imread(base_dir4+img)\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    X_q4.append(image)\n",
        "with open(base_dir+'q4', 'wb') as file_pi1:\n",
        "    pickle.dump(X_q4, file_pi1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###load Images from Drive"
      ],
      "metadata": {
        "id": "2TsORrYb-zrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(base_dir+'q1', 'rb') as f:\n",
        "    X_q1 = pickle.load(f)\n",
        "\n",
        "with open(base_dir+'q2', 'rb') as f:\n",
        "    X_q2 = pickle.load(f)\n",
        "\n",
        "with open(base_dir+'q3', 'rb') as f:\n",
        "    X_q3 = pickle.load(f)\n",
        "\n",
        "with open(base_dir+'q4', 'rb') as f:\n",
        "    X_q4 = pickle.load(f)"
      ],
      "metadata": {
        "id": "jRIMYqeE-5Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVONSxpa0JrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bQz1mtE6n9B"
      },
      "outputs": [],
      "source": [
        "Images_data = X_q1+X_q2+X_q3+X_q4\n",
        "import pickle\n",
        "base_dir = '/content/gdrive/My Drive/08_emotion/'\n",
        "with open(base_dir+'Images_40clips_32subjects', 'wb') as file_pi1:\n",
        "    pickle.dump(Images_data, file_pi1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load from drive"
      ],
      "metadata": {
        "id": "QURRauYhA_Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(base_dir+'Images_40clips_32subjects', 'rb') as f:\n",
        "    Images_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "QJKK5vsMAyTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kotZatHmQQWm"
      },
      "outputs": [],
      "source": [
        "q1 = 5760*[0]\n",
        "q2 = 5760*[1]\n",
        "q3 = 5760*[2]\n",
        "q4 = 5760*[3]\n",
        "\n",
        "labels = q1+q2+q3+q4\n",
        "#labels\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Images_data,labels,test_size=0.1, random_state=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llEr4QpRIqpo"
      },
      "source": [
        "# **TL-EfficientNetb0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4vHdJLBQgjz",
        "outputId": "0d17ea87-eee7-4d80-9ba9-620ce91ff094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_model():\n",
        "  conv_base = tf.keras.applications.EfficientNetB0(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  conv_base.trainable = True\n",
        "  '''set_trainable = True\n",
        "  for layer in conv_base.layers:\n",
        "      if layer.name == 'block5_conv3':\n",
        "          set_trainable = True\n",
        "      if set_trainable:\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "          layer.trainable = False\n",
        "  print(\"Model edited\")'''\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten())\n",
        "  #model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(128)) #,kernel_regularizer=regularizers.l1_l2(l1=1e-6, l2=1e-5),bias_regularizer=regularizers.l2(1e-4),activity_regularizer=regularizers.l2(1e-5)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  #model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Activation('sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=3e-5),metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "\n",
        "fold_count=0\n",
        "hist_VGG16 = {}\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "for train_index, val_index in skf.split(X_train,Y_train):\n",
        "    fold_count = fold_count + 1\n",
        "    #if fold_count <7:\n",
        "    #  continue;\n",
        "    x_train, x_val = np.array(X_train)[train_index], np.array(X_train)[val_index]\n",
        "    y_train, y_val = np.array(Y_train)[train_index], np.array(Y_train)[val_index]\n",
        "\n",
        "    model = create_model()\n",
        "    history = model.fit(x_train,y_train,epochs=50, batch_size=16, validation_data = (x_val,y_val), shuffle=True)\n",
        "    plot_results(history)\n",
        "    \n",
        "    hist_VGG16['fold'+str(fold_count)+'_history']=history.history\n",
        "    hist_VGG16['fold'+str(fold_count)+'_results'] = calculate_test_accuracy()\n",
        "    print('fold_'+str(fold_count)+' done!')\n",
        "    with open(base_dir+'results/hist_VGG16_fold_'+str(fold_count), 'wb') as file_pi1:\n",
        "       pickle.dump(hist_VGG16, file_pi1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JAra1Mdsgrl"
      },
      "source": [
        "# **Prepare the video Samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrrc68wDUaiD"
      },
      "outputs": [],
      "source": [
        "# prepare video samples from scatch or from previously loaded Images_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX12zDCHU0bU"
      },
      "outputs": [],
      "source": [
        "# Alternatively use previously saved Images_data\n",
        "import pickle\n",
        "import numpy as np\n",
        "base_dir = '/content/gdrive/My Drive/08_emotion/'\n",
        "Images_data = pickle.load(open(base_dir+'Images_40clips_32subjects', \"rb\" ))\n",
        "\n",
        "X_q1 = Images_data[:5760]\n",
        "X_q2 = Images_data[5760:2*5760]\n",
        "X_q3 = Images_data[2*5760:3*5760]\n",
        "X_q4 = Images_data[3*5760:4*5760]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Images_data = []"
      ],
      "metadata": {
        "id": "gqEJ39UKcVLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpTKcu-ismk_",
        "outputId": "58a9fac8-cb07-464d-dc06-68470911747d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 4, 224, 224, 3)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "video_q1 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 4\n",
        "for subj in range(32*40): # number of subjects\n",
        "  for img in range(4): #stride 1/number of images of each subject\n",
        "       if img+frame_length>4:\n",
        "          break\n",
        "       tmp_video = X_q1[subj*4+img:subj*4+img+frame_length]\n",
        "       video_q1.append(tmp_video)\n",
        "print(np.shape(video_q1))\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKjGvmplN8Vt",
        "outputId": "054de11a-ffab-4636-f3cb-206d73e2d8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 4, 224, 224, 3)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "video_q2 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 4\n",
        "for subj in range(32*40): # number of subjects\n",
        "  for img in range(4): #stride 1/number of images of each subject\n",
        "       if img+frame_length>4:\n",
        "          break\n",
        "       tmp_video = X_q2[subj*4+img:subj*4+img+frame_length]\n",
        "       video_q2.append(tmp_video)\n",
        "print(np.shape(video_q2))\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs04GrZCOCRI",
        "outputId": "0bda996f-ee41-4c96-c113-18c12a7922c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 4, 224, 224, 3)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "video_q3 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 4\n",
        "for subj in range(32*40): # number of subjects\n",
        "  for img in range(4): #stride 1/number of images of each subject\n",
        "       if img+frame_length>4:\n",
        "          break\n",
        "       tmp_video = X_q3[subj*4+img:subj*4+img+frame_length]\n",
        "       video_q3.append(tmp_video)\n",
        "print(np.shape(video_q3))\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VzBUwT-OILH",
        "outputId": "4246f8dc-bf67-4360-fde7-f4e83bc7b568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 4, 224, 224, 3)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "video_q4 = []\n",
        "subj = 0\n",
        "stride = 1\n",
        "init_frame = 0\n",
        "frame_length = 4\n",
        "for subj in range(32*40): # number of subjects\n",
        "  for img in range(4): #stride 1/number of images of each subject\n",
        "       if img+frame_length>4:\n",
        "          break\n",
        "       tmp_video = X_q4[subj*4+img:subj*4+img+frame_length]\n",
        "       video_q4.append(tmp_video)\n",
        "print(np.shape(video_q4))\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT901cAbsWNo"
      },
      "outputs": [],
      "source": [
        "video_data=video_q1+video_q2+video_q3+video_q4\n",
        "video_q1,video_q2,video_q3,video_q4,X_q1,X_q2,X_q3,X_q4 = [],[],[],[],[],[],[],[]\n",
        "#del video_sz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJRTvOSGtYW-"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "base_dir = '/content/gdrive/My Drive/08_emotion/'\n",
        "with open(base_dir+'video_data', 'wb') as file_pi1:\n",
        "    pickle.dump(video_data, file_pi1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iRsWkcs7_WC"
      },
      "source": [
        "### **load video from drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vhaplIU7_WD"
      },
      "outputs": [],
      "source": [
        "# Alternatively use previously saved video_data\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "base_dir = '/content/gdrive/My Drive/08_emotion/'\n",
        "video_data = pickle.load(open(base_dir+'video_data', \"rb\" ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGpO1UIA7_WD"
      },
      "outputs": [],
      "source": [
        "q1 = 1280*[0]\n",
        "q2 = 1280*[1]\n",
        "q3 = 1280*[2]\n",
        "q4 = 1280*[3]\n",
        "\n",
        "labels = q1+q2+q3+q4\n",
        "#labels\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(video_data,labels,test_size=0.1, random_state=8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_data=[]"
      ],
      "metadata": {
        "id": "xItMaJq4gADt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGwvqfRqxExB"
      },
      "source": [
        "**bold text**# **EfficientnetB0-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  conv_base = tf.keras.applications.EfficientNetB0(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  conv_base.trainable = True\n",
        "  '''set_trainable = False\n",
        "  for layer in conv_base.layers:\n",
        "      if layer.name == 'block5_conv2':\n",
        "          set_trainable = True\n",
        "      if set_trainable:\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "          layer.trainable = False\n",
        "  print(\"Model edited\")'''\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(32)) #,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4),activity_regularizer=regularizers.l2(1e-5)))\n",
        "  \n",
        "  '''model.add(BatchNormalization())\n",
        "  model.add(layers.Activation('sigmoid'))'''\n",
        "  cnn = models.Model(model.input, model.output)\n",
        "\n",
        "  seq_input = Input(shape=(4,224,224,3))\n",
        "  encoded_sequence = TimeDistributed(cnn)(seq_input)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=True))(encoded_sequence)\n",
        "  \n",
        "  #encoded_sequence = Dropout(rate=0.1)(encoded_sequence)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=False))(encoded_sequence)\n",
        "  out = Dense(4, activation=\"softmax\")(encoded_sequence)\n",
        "  #out = Convolution1D(1, kernel_size=1, activation=\"sigmoid\", padding=\"same\")(encoded_sequence)\n",
        "\n",
        "  cnn_lstm = models.Model(seq_input, out)\n",
        "  model.summary()\n",
        "  \n",
        "  cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4),metrics=['acc'])\n",
        "  if fold_count == 1:\n",
        "    cnn_lstm.summary()\n",
        "  return cnn_lstm\n",
        "\n",
        "fold_count=0\n",
        "hist_EfficientNetB0LSTM = {}\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "for train_index, val_index in skf.split(X_train,Y_train):\n",
        "    fold_count = fold_count + 1\n",
        "    #if fold_count < 8:\n",
        "    #  continue;\n",
        "    x_train, x_val = np.array(X_train)[train_index], np.array(X_train)[val_index]\n",
        "    y_train, y_val = np.array(Y_train)[train_index], np.array(Y_train)[val_index]\n",
        "    y_train = np_utils.to_categorical(y_train)\n",
        "    y_val = np_utils.to_categorical(y_val)\n",
        "\n",
        "    model = create_model()\n",
        "    from keras.utils.vis_utils import plot_model\n",
        "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=False)\n",
        "    history = model.fit(x_train,y_train,epochs=50, batch_size=10, validation_data = (x_val,y_val), shuffle=True)\n",
        "    plot_results(history)\n",
        "    \n",
        "    hist_EfficientNetB0LSTM['fold'+str(fold_count)+'_history']= history.history\n",
        "    hist_EfficientNetB0LSTM['fold'+str(fold_count)+'_results'] = calculate_test_accuracy()\n",
        "    print('fold_'+str(fold_count)+' done!')\n",
        "    with open(base_dir+'hist_EfficientNetB0LSTM_fold_'+str(fold_count), 'wb') as file_pi1:\n",
        "        pickle.dump(hist_EfficientNetB0LSTM, file_pi1)"
      ],
      "metadata": {
        "id": "IiI9K93PSPlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(32)) #,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4),activity_regularizer=regularizers.l2(1e-5)))\n",
        "  \n",
        "  '''model.add(BatchNormalization())\n",
        "  model.add(layers.Activation('sigmoid'))'''\n",
        "  cnn = models.Model(model.input, model.output)\n",
        "\n",
        "  seq_input = Input(shape=(4,224,224,3))\n",
        "  encoded_sequence = TimeDistributed(cnn)(seq_input)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=True))(encoded_sequence)\n",
        "  \n",
        "  #encoded_sequence = Dropout(rate=0.1)(encoded_sequence)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=False))(encoded_sequence)\n",
        "  out = Dense(4, activation=\"softmax\")(encoded_sequence)\n",
        "  #out = Convolution1D(1, kernel_size=1, activation=\"sigmoid\", padding=\"same\")(encoded_sequence)\n",
        "\n",
        "  cnn_lstm = models.Model(seq_input, out)\n",
        "  model.summary()"
      ],
      "metadata": {
        "id": "Q5O303kkSZ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_results = {}\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "predicted_probas = model.predict(np.array(X_test),steps = len(X_test))\n",
        "fold_results['predicted_probas'] = predicted_probas\n",
        "maxindex = predicted_probas[i,:].argmax()\n",
        "binary_prediction=[]\n",
        "binary_prediction0=[]\n",
        "for i in range(len(predicted_probas)):\n",
        "    binary_prediction0 = 1 if predicted_probas[i,:].argmax()>=0.5 else 0\n",
        "    # binary_prediction0 = np.round(predicted_probas)\n",
        "    binary_prediction.append(binary_prediction0)\n",
        "fold_results['binary_prediction'] = binary_prediction\n",
        "conf_mat = confusion_matrix(binary_prediction, Y_test)\n",
        "fold_results['confusion_matrix'] = conf_mat\n",
        "acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
        "fold_results['test_accuracy'] = acc\n",
        "print('Overall accuracy: {:.2f} %'.format(acc*100))"
      ],
      "metadata": {
        "id": "HLHainNnSkEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probas"
      ],
      "metadata": {
        "id": "3c6D5i2_Sp5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  conv_base = tf.keras.applications.EfficientNetB0(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
        "  conv_base.trainable = True\n",
        "  '''set_trainable = False\n",
        "  for layer in conv_base.layers:\n",
        "      if layer.name == 'block5_conv2':\n",
        "          set_trainable = True\n",
        "      if set_trainable:\n",
        "          layer.trainable = True\n",
        "      else:\n",
        "          layer.trainable = False\n",
        "  print(\"Model edited\")'''\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(32)) #,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4),activity_regularizer=regularizers.l2(1e-5)))\n",
        "  \n",
        "  '''model.add(BatchNormalization())\n",
        "  model.add(layers.Activation('sigmoid'))'''\n",
        "  cnn = models.Model(model.input, model.output)\n",
        "\n",
        "  seq_input = Input(shape=(4,224,224,3))\n",
        "  encoded_sequence = TimeDistributed(cnn)(seq_input)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=True))(encoded_sequence)\n",
        "  \n",
        "  #encoded_sequence = Dropout(rate=0.1)(encoded_sequence)\n",
        "  encoded_sequence = Bidirectional(LSTM(32, return_sequences=False))(encoded_sequence)\n",
        "  out = Dense(4, activation=\"softmax\")(encoded_sequence)\n",
        "  #out = Convolution1D(1, kernel_size=1, activation=\"sigmoid\", padding=\"same\")(encoded_sequence)\n",
        "\n",
        "  cnn_lstm = models.Model(seq_input, out)\n",
        "  cnn_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4),metrics=['acc'])\n",
        "  if fold_count == 1:\n",
        "    cnn_lstm.summary()\n",
        "  return cnn_lstm\n",
        "\n",
        "fold_count=0\n",
        "hist_EfficientNetB0LSTM = {}\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "for train_index, val_index in skf.split(X_train,Y_train):\n",
        "    fold_count = fold_count + 1\n",
        "    #if fold_count < 8:\n",
        "    #  continue;\n",
        "    x_train, x_val = np.array(X_train)[train_index], np.array(X_train)[val_index]\n",
        "    y_train, y_val = np.array(Y_train)[train_index], np.array(Y_train)[val_index]\n",
        "    y_train = np_utils.to_categorical(y_train)\n",
        "    y_val = np_utils.to_categorical(y_val)\n",
        "\n",
        "    model = create_model()\n",
        "    from keras.utils.vis_utils import plot_model\n",
        "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=False)\n",
        "    history = model.fit(x_train,y_train,epochs=50, batch_size=10, validation_data = (x_val,y_val), shuffle=True)\n",
        "    plot_results(history)\n",
        "    \n",
        "    hist_EfficientNetB0LSTM['fold'+str(fold_count)+'_history']= history.history\n",
        "    hist_EfficientNetB0LSTM['fold'+str(fold_count)+'_results'] = calculate_test_accuracy()\n",
        "    print('fold_'+str(fold_count)+' done!')\n",
        "    with open(base_dir+'hist_EfficientNetB0LSTM_fold_'+str(fold_count), 'wb') as file_pi1:\n",
        "        pickle.dump(hist_EfficientNetB0LSTM, file_pi1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OkMxGIydwxO",
        "outputId": "896b4d52-ac16-4122-b958-7bfb31e4443a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 4, 224, 224, 3)]  0         \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 4, 32)            6056643   \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 4, 64)            16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               24832     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,098,375\n",
            "Trainable params: 6,056,352\n",
            "Non-trainable params: 42,023\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "219/415 [==============>...............] - ETA: 1:27 - loss: 1.2652 - acc: 0.4479"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bwrH5aaNeSFg"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}